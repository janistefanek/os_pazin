# This file contains the different endpoints your bot can use.

# Server where the models are pulled from.
# https://rasa.com/docs/rasa-pro/production/model-storage#fetching-models-from-a-server

#models:
#  url: http://my-server.com/models/default_core@latest
#  wait_time_between_pulls:  10   # [optional](default: 100)

# Server which runs your custom actions.
# https://rasa.com/docs/rasa-pro/concepts/custom-actions

action_endpoint:
  actions_module: "actions"

# Tracker store which is used to store the conversations.
# By default the conversations are stored in memory.
# https://rasa.com/docs/rasa-pro/production/tracker-stores

#tracker_store:
#    type: redis
#    url: <host of the redis instance, e.g. localhost>
#    port: <port of your redis instance, usually 6379>
#    db: <number of your database within redis, e.g. 0>
#    password: <password used for authentication>
#    use_ssl: <whether or not the communication is encrypted, default false>

#tracker_store:
#    type: mongod
#    url: <url to your mongo instance, e.g. mongodb://localhost:27017>
#    db: <name of the db within your mongo instance, e.g. rasa>
#    username: <username used for authentication>
#    password: <password used for authentication>

# Event broker which all conversation events should be streamed to.
# https://rasa.com/docs/rasa-pro/production/event-brokers

#event_broker:
#  url: localhost
#  username: username
#  password: password
#  queue: queue

# Contextual rephraser is DISABLED to prevent extra LLM-generated replies.
# To re-enable, restore the block below and ensure credentials are set.
nlg:
  type: rephrase
  llm:
    model_group: llama_3_1_8b_model

model_groups:
   - id: llama_3_1_8b_model
     models:
       - provider: rasa
         model: rasa/command-generator-llama-3.1-8b-instruct
         api_base: https://tutorial-llm.rasa.ai
